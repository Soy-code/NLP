{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm \n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 벡터화 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            review_df (pandas.DataFrame): 데이터셋\n",
    "            vectorizer (ReviewVectorizer): ReviewVectorizer 객체\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\" 데이터셋을 로드하고 새로운 ReviewVectorizer 객체를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            review_csv (str): 데이터셋의 위치\n",
    "        반환값:\n",
    "            ReviewDataset의 인스턴스\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split=='train']\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
    "        \"\"\" 데이터셋을 로드하고 새로운 ReviewVectorizer 객체를 만듭니다.\n",
    "        캐시된 ReviewVectorizer 객체를 재사용할 때 사용합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            review_csv (str): 데이터셋의 위치\n",
    "            vectorizer_filepath (str): ReviewVectorizer 객체의 저장 위치\n",
    "        반환값:\n",
    "            ReviewDataset의 인스턴스\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(review_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\" 파일에서 ReviewVectorizer 객체를 로드하는 정적 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): 직렬화된 ReviewVectorizer 객체의 위치\n",
    "        반환값:\n",
    "            ReviewVectorizer의 인스턴스\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\" ReviewVectorizer 객체를 json 형태로 디스크에 저장합니다\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): ReviewVectorizer 객체의 저장 위치\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" 벡터 변환 객체를 반환합니다 \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" 데이터프레임에 있는 열을 사용해 분할 세트를 선택합니다 \n",
    "        \n",
    "        매개변수:\n",
    "            split (str): \"train\", \"val\", \"test\" 중 하나\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" 파이토치 데이터셋의 주요 진입 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            index (int): 데이터 포인트의 인덱스\n",
    "        반환값:\n",
    "            데이터 포인트의 특성(x_data)과 레이블(y_target)로 이루어진 딕셔너리\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        review_vector = \\\n",
    "            self._vectorizer.vectorize(row.review)\n",
    "\n",
    "        rating_index = \\\n",
    "            self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "        return {'x_data': review_vector,\n",
    "                'y_target': rating_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\" 배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다\n",
    "        \n",
    "        매개변수:\n",
    "            batch_size (int)\n",
    "        반환값:\n",
    "            배치 개수\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "- 정수-토큰 매핑을 수행\n",
    "- 텍스트 토큰과 클래스 레이블을 정수로 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" 매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
    "            add_unk (bool): UNK 토큰을 추가할지 지정하는 플래그\n",
    "            unk_token (str): Vocabulary에 추가할 UNK 토큰\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
    "\n",
    "        매개변수:\n",
    "            token (str): Vocabulary에 추가할 토큰\n",
    "        반환값:\n",
    "            index (int): 토큰에 상응하는 정수\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\" 토큰 리스트를 Vocabulary에 추가합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            tokens (list): 문자열 토큰 리스트\n",
    "        반환값:\n",
    "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
    "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        노트:\n",
    "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
    "            `unk_index`가 0보다 커야 합니다.\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
    "        \n",
    "        매개변수: \n",
    "            index (int): 찾을 인덱스\n",
    "        반환값:\n",
    "            token (str): 인텍스에 해당하는 토큰\n",
    "        에러:\n",
    "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"Vocabulary에 인덱스(%d)가 없습니다.\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer\n",
    "* 입력 데이터 포인트의 토큰을 순회하면서 각 토큰을 정수로 바꿈\n",
    "* 반복 과정의 결과는 벡터\n",
    "* vectorizer에서 만든 벡터는 항상 길이가 같아야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            review_vocab (Vocabulary): 단어를 정수에 매핑하는 Vocabulary\n",
    "            rating_vocab (Vocabulary): 클래스 레이블을 정수에 매핑하는 Vocabulary\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        \"\"\" 리뷰에 대한 웟-핫 벡터를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            review (str): 리뷰\n",
    "        반환값:\n",
    "            one_hot (np.ndarray): 원-핫 벡터\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        \n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25):\n",
    "        \"\"\" 데이터셋 데이터프레임에서 Vectorizer 객체를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            review_df (pandas.DataFrame): 리뷰 데이터셋\n",
    "            cutoff (int): 빈도 기반 필터링 설정값\n",
    "        반환값:\n",
    "            ReviewVectorizer 객체\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # 점수를 추가합니다\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "\n",
    "        # count > cutoff인 단어를 추가합니다\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "               \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "\n",
    "        return cls(review_vocab, rating_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" 직렬화된 딕셔너리에서 ReviewVectorizer 객체를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            contents (dict): 직렬화된 딕셔너리\n",
    "        반환값:\n",
    "            ReviewVectorizer 클래스 객체\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab =  Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "\n",
    "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" 캐싱을 위해 직렬화된 딕셔너리를 만듭니다\n",
    "        \n",
    "        반환값:\n",
    "            contents (dict): 직렬화된 딕셔너리\n",
    "        \"\"\"\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
    "                'rating_vocab': self.rating_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "* 벡터로 변환한 데이터 포인터 모으기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    파이토치 DataLoader를 감싸고 있는 제너레이터 함수.\n",
    "    걱 텐서를 지정된 장치로 이동합니다.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReviewClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\" 간단한 퍼셉트론 기반 분류기 \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            num_features (int): 입력 특성 벡트의 크기\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, \n",
    "                             out_features=1)\n",
    "\n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\" 분류기의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 데이터 텐서 \n",
    "                x_in.shape는 (batch, num_features)입니다.\n",
    "            apply_sigmoid (bool): 시그모이드 활성화 함수를 위한 플래그\n",
    "                크로스-엔트로피 손실을 사용하려면 False로 지정합니다\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape은 (batch,)입니다.\n",
    "        \"\"\"\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 경로: \n",
      "\t./model_storage/ch3/yelp\\vectorizer.json\n",
      "\t./model_storage/ch3/yelp\\model.pth\n",
      "CUDA 사용 여부: False\n"
     ]
    }
   ],
   "source": [
    "# utils\n",
    "def set_seed_everywhere(seed, cuda) :\n",
    "    np.random.seed(seed) \n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath) :\n",
    "    if not os.path.exists(dirpath) :\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "# parameters\n",
    "args = Namespace(\n",
    "    # 날짜와 경로 정보\n",
    "    frequency_cutoff = 25, \n",
    "    model_state_file = 'model.pth', \n",
    "    review_csv = 'C:/Users/thdus/work/NLP(old)/data/reviews_with_splits_lite.csv', \n",
    "    save_dir = './model_storage/ch3/yelp',\n",
    "    vectorizer_file = 'vectorizer.json',\n",
    "    # 훈련 하이어 파라미터\n",
    "    batch_size = 128, \n",
    "    early_stopping_criteria = 5,\n",
    "    learning_rate = 0.001,\n",
    "    num_epochs = 100,\n",
    "    seed = 1337,\n",
    "    # 실행 옵션\n",
    "    catch_keyboard_interrupt = True, \n",
    "    cuda = True, \n",
    "    expand_filepaths_to_save_dir = True, \n",
    "    reload_from_files =False, \n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir :\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(\"파일 경로: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# cuda check\n",
    "if not torch.cuda.is_available() :\n",
    "    args.cuda = False\n",
    "\n",
    "print('CUDA 사용 여부: {}'.format(args.cuda))\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "\n",
    "# seed 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# directory 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 상태를 초기화 하는 함수\n",
    "def make_train_state(args) :\n",
    "    return {'stop_early' : False, \n",
    "            'early_stopping_step' : 0,\n",
    "            'early_stopping_best_val' : 1e8, \n",
    "            'learning_rate' : args.learning_rate, \n",
    "            'epoch_index' : 0, \n",
    "            'train_loss': [], \n",
    "            'train_acc' :[], \n",
    "            'val_loss' : [],\n",
    "            'val_acc' : [], \n",
    "            'test_loss' : [],\n",
    "            'test_acc' : [],\n",
    "            'model_filename' : args.model_state_file\n",
    "            }\n",
    "\n",
    "# 훈련 상태를 업데이트 하는 함수\n",
    "def update_train_state(args, model, train_state) :\n",
    "    \"\"\" 훈련 상태를 업데이트합니다.\n",
    "\n",
    "    Components:\n",
    "     - 조기 종료: 과대 적합 방지\n",
    "     - 모델 체크포인트: 더 나은 모델을 저장합니다\n",
    "\n",
    "    :param args: 메인 매개변수\n",
    "    :param model: 훈련할 모델\n",
    "    :param train_state: 훈련 상태를 담은 딕셔너리\n",
    "    :returns:\n",
    "        새로운 훈련 상태\n",
    "    \"\"\"\n",
    "    # 적어도 한번 모델을 저장\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "    \n",
    "    # 성능이 향상되면 모델을 저장\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "        # 손실이 나빠지면 \n",
    "        if loss_t >= train_state['early_stopping_best_val'] :\n",
    "            # 조기종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else :\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val'] :\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "            train_state['early_stopping_step'] = 0 \n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "    \n",
    "    return train_state\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/thdus/work/NLP(old)/data/reviews_with_splits_lite.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.review_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋을 로드하고 vectorizer를 만듭니다\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files :\n",
    "    # 체크포인트에서 훈련을 다시 시작\n",
    "    print(\"데이터셋과 Vectorizer를 로드합니다\")\n",
    "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv, \n",
    "                                                             args.vectorizer_file)\n",
    "else :\n",
    "    print(\"데이터셋을 로드하고 vectorizer를 만듭니다\")\n",
    "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = args.learning_rate)\n",
    "# 스케줄러\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                 mode = 'min', \n",
    "                                                 factor = 0.5, \n",
    "                                                 patience = 1)\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': False,\n",
       " 'early_stopping_step': 0,\n",
       " 'early_stopping_best_val': 100000000.0,\n",
       " 'learning_rate': 0.001,\n",
       " 'epoch_index': 0,\n",
       " 'train_loss': [],\n",
       " 'train_acc': [],\n",
       " 'val_loss': [],\n",
       " 'val_acc': [],\n",
       " 'test_loss': [],\n",
       " 'test_acc': [],\n",
       " 'model_filename': './model_storage/ch3/yelp\\\\model.pth'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f958a8b0c144f25bfd2a30325fc5c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d1e8e8e31f4102beb797c3e1f8874f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb1ebfe0e364409be481644639fe386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련 반복\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    # 에포크 수만큼 for 반복문 실행 \n",
    "    for epoch_index in range(args.num_epochs):\n",
    "\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        # 훈련 세트에 대한 순회\n",
    "        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        # 모델이 훈련모드에 있고, 모델 파라미터를 수정할 수 있다고 지정\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 훈련 과정은 5단계로 이루어집니다\n",
    "\n",
    "            # --------------------------------------\n",
    "            # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 단계 2. 출력을 계산합니다\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # 단계 3. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "            loss.backward()\n",
    "\n",
    "            # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            \n",
    "            # 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # 진행 바 업데이트\n",
    "            train_bar.set_postfix(loss=running_loss, \n",
    "                                  acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 검증 세트에 대한 순회\n",
    "\n",
    "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # 단계 1. 출력을 계산합니다\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # 단계 2. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 3. 정확도를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, \n",
    "                                acc=running_acc, \n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 0.224\n",
      "테스트 정확도: 91.05\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    \"\"\" 리뷰 점수 예측하기\n",
    "    \n",
    "    매개변수:\n",
    "        review (str): 리뷰 텍스트\n",
    "        classifier (ReviewClassifier): 훈련된 모델\n",
    "        vectorizer (ReviewVectorizer): Vectorizer 객체\n",
    "        decision_threshold (float): 클래스를 나눌 결정 경계\n",
    "    \"\"\"\n",
    "    review = preprocess_text(review)\n",
    "    \n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    \n",
    "    probability_value = torch.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "\n",
    "    return vectorizer.rating_vocab.lookup_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a pretty awesome book -> positive\n"
     ]
    }
   ],
   "source": [
    "test_review = \"this is a pretty awesome book\"\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7356])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_weights = classifier.fc1.weight.detach()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0569,  0.0771,  0.0392,  ...,  0.5691,  0.7088,  0.4740])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 정렬\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0569,  0.0771,  0.0392,  ...,  0.5691,  0.7088,  0.4740])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([ 1.4870,  1.4499,  1.3879,  ..., -1.6095, -1.7450, -1.8720]),\n",
       "indices=tensor([ 811, 1176, 6169,  ...,  458,  587,  103]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sort(fc1_weights, dim=0, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[103,\n",
       " 587,\n",
       " 458,\n",
       " 1007,\n",
       " 801,\n",
       " 420,\n",
       " 1097,\n",
       " 252,\n",
       " 2061,\n",
       " 1241,\n",
       " 2948,\n",
       " 2425,\n",
       " 1646,\n",
       " 1096,\n",
       " 1788,\n",
       " 2448,\n",
       " 621,\n",
       " 646,\n",
       " 786,\n",
       " 1098,\n",
       " 2759,\n",
       " 2989,\n",
       " 2731,\n",
       " 837,\n",
       " 97,\n",
       " 899,\n",
       " 802,\n",
       " 2820,\n",
       " 638,\n",
       " 383,\n",
       " 211,\n",
       " 991,\n",
       " 413,\n",
       " 1753,\n",
       " 1867,\n",
       " 584,\n",
       " 4739,\n",
       " 272,\n",
       " 412,\n",
       " 3368,\n",
       " 2797,\n",
       " 1764,\n",
       " 1825,\n",
       " 5155,\n",
       " 1400,\n",
       " 363,\n",
       " 1217,\n",
       " 2732,\n",
       " 3809,\n",
       " 747,\n",
       " 1980,\n",
       " 324,\n",
       " 1229,\n",
       " 931,\n",
       " 2878,\n",
       " 3285,\n",
       " 762,\n",
       " 2068,\n",
       " 264,\n",
       " 5050,\n",
       " 4296,\n",
       " 1844,\n",
       " 1770,\n",
       " 2507,\n",
       " 6496,\n",
       " 3492,\n",
       " 5683,\n",
       " 2713,\n",
       " 1596,\n",
       " 2467,\n",
       " 2325,\n",
       " 2037,\n",
       " 1999,\n",
       " 3259,\n",
       " 1220,\n",
       " 6360,\n",
       " 2400,\n",
       " 5824,\n",
       " 1676,\n",
       " 3487,\n",
       " 966,\n",
       " 3428,\n",
       " 4617,\n",
       " 176,\n",
       " 337,\n",
       " 2796,\n",
       " 301,\n",
       " 2960,\n",
       " 2007,\n",
       " 2990,\n",
       " 2816,\n",
       " 5855,\n",
       " 1768,\n",
       " 5774,\n",
       " 4928,\n",
       " 563,\n",
       " 2431,\n",
       " 1078,\n",
       " 67,\n",
       " 846,\n",
       " 3298,\n",
       " 2414,\n",
       " 3717,\n",
       " 2077,\n",
       " 3929,\n",
       " 3245,\n",
       " 484,\n",
       " 4506,\n",
       " 5307,\n",
       " 3293,\n",
       " 1209,\n",
       " 4373,\n",
       " 1001,\n",
       " 3208,\n",
       " 3788,\n",
       " 457,\n",
       " 2161,\n",
       " 526,\n",
       " 1955,\n",
       " 439,\n",
       " 2812,\n",
       " 3440,\n",
       " 1758,\n",
       " 4599,\n",
       " 1919,\n",
       " 4302,\n",
       " 5044,\n",
       " 3619,\n",
       " 4641,\n",
       " 4284,\n",
       " 3679,\n",
       " 849,\n",
       " 4614,\n",
       " 127,\n",
       " 3812,\n",
       " 3479,\n",
       " 411,\n",
       " 4548,\n",
       " 774,\n",
       " 4898,\n",
       " 1230,\n",
       " 1153,\n",
       " 1306,\n",
       " 1549,\n",
       " 4371,\n",
       " 2877,\n",
       " 2185,\n",
       " 5151,\n",
       " 2098,\n",
       " 3122,\n",
       " 6067,\n",
       " 317,\n",
       " 3092,\n",
       " 6669,\n",
       " 2725,\n",
       " 2385,\n",
       " 1861,\n",
       " 2727,\n",
       " 3867,\n",
       " 999,\n",
       " 1404,\n",
       " 687,\n",
       " 5938,\n",
       " 1514,\n",
       " 142,\n",
       " 2834,\n",
       " 691,\n",
       " 4984,\n",
       " 229,\n",
       " 4272,\n",
       " 939,\n",
       " 2104,\n",
       " 1178,\n",
       " 3517,\n",
       " 3223,\n",
       " 4062,\n",
       " 2951,\n",
       " 2558,\n",
       " 1510,\n",
       " 3811,\n",
       " 1626,\n",
       " 1899,\n",
       " 1152,\n",
       " 1876,\n",
       " 294,\n",
       " 6179,\n",
       " 3340,\n",
       " 5604,\n",
       " 1806,\n",
       " 351,\n",
       " 1725,\n",
       " 1062,\n",
       " 4656,\n",
       " 1115,\n",
       " 5783,\n",
       " 5576,\n",
       " 695,\n",
       " 5500,\n",
       " 5505,\n",
       " 3837,\n",
       " 3157,\n",
       " 1963,\n",
       " 38,\n",
       " 970,\n",
       " 5517,\n",
       " 4104,\n",
       " 3960,\n",
       " 1622,\n",
       " 5407,\n",
       " 1326,\n",
       " 2160,\n",
       " 5048,\n",
       " 3688,\n",
       " 2212,\n",
       " 6615,\n",
       " 1902,\n",
       " 1530,\n",
       " 1182,\n",
       " 5888,\n",
       " 183,\n",
       " 2793,\n",
       " 2419,\n",
       " 2174,\n",
       " 2108,\n",
       " 5691,\n",
       " 2762,\n",
       " 3244,\n",
       " 2880,\n",
       " 1850,\n",
       " 6528,\n",
       " 2243,\n",
       " 978,\n",
       " 2536,\n",
       " 4116,\n",
       " 6193,\n",
       " 1125,\n",
       " 4887,\n",
       " 4568,\n",
       " 2267,\n",
       " 4643,\n",
       " 5519,\n",
       " 1584,\n",
       " 3591,\n",
       " 757,\n",
       " 6644,\n",
       " 5928,\n",
       " 1655,\n",
       " 1455,\n",
       " 339,\n",
       " 4710,\n",
       " 1895,\n",
       " 2341,\n",
       " 3597,\n",
       " 1407,\n",
       " 1785,\n",
       " 6529,\n",
       " 1357,\n",
       " 781,\n",
       " 1491,\n",
       " 5413,\n",
       " 215,\n",
       " 232,\n",
       " 6209,\n",
       " 6897,\n",
       " 4494,\n",
       " 3938,\n",
       " 2608,\n",
       " 6004,\n",
       " 5191,\n",
       " 4079,\n",
       " 2096,\n",
       " 1033,\n",
       " 831,\n",
       " 6670,\n",
       " 1526,\n",
       " 4361,\n",
       " 340,\n",
       " 3402,\n",
       " 3657,\n",
       " 4894,\n",
       " 5561,\n",
       " 6275,\n",
       " 546,\n",
       " 1930,\n",
       " 468,\n",
       " 4538,\n",
       " 2145,\n",
       " 866,\n",
       " 4983,\n",
       " 2430,\n",
       " 552,\n",
       " 315,\n",
       " 3593,\n",
       " 3193,\n",
       " 4972,\n",
       " 5269,\n",
       " 5744,\n",
       " 296,\n",
       " 1006,\n",
       " 6965,\n",
       " 6369,\n",
       " 47,\n",
       " 455,\n",
       " 1020,\n",
       " 1786,\n",
       " 1433,\n",
       " 2644,\n",
       " 1700,\n",
       " 4376,\n",
       " 5328,\n",
       " 2027,\n",
       " 2327,\n",
       " 9,\n",
       " 1669,\n",
       " 1197,\n",
       " 4393,\n",
       " 1107,\n",
       " 2466,\n",
       " 1174,\n",
       " 2021,\n",
       " 2656,\n",
       " 2227,\n",
       " 1650,\n",
       " 1201,\n",
       " 3084,\n",
       " 5161,\n",
       " 3483,\n",
       " 401,\n",
       " 1778,\n",
       " 254,\n",
       " 2864,\n",
       " 4836,\n",
       " 3452,\n",
       " 647,\n",
       " 3411,\n",
       " 506,\n",
       " 4214,\n",
       " 5512,\n",
       " 3363,\n",
       " 1990,\n",
       " 3015,\n",
       " 1225,\n",
       " 3329,\n",
       " 244,\n",
       " 449,\n",
       " 2041,\n",
       " 3678,\n",
       " 1422,\n",
       " 3640,\n",
       " 5809,\n",
       " 3312,\n",
       " 4103,\n",
       " 3914,\n",
       " 1459,\n",
       " 4476,\n",
       " 6924,\n",
       " 5733,\n",
       " 4531,\n",
       " 3969,\n",
       " 3510,\n",
       " 5176,\n",
       " 1116,\n",
       " 5181,\n",
       " 2473,\n",
       " 470,\n",
       " 5461,\n",
       " 4525,\n",
       " 6424,\n",
       " 1478,\n",
       " 3894,\n",
       " 1888,\n",
       " 5121,\n",
       " 3354,\n",
       " 7020,\n",
       " 2090,\n",
       " 2359,\n",
       " 242,\n",
       " 4564,\n",
       " 2016,\n",
       " 5316,\n",
       " 6234,\n",
       " 1591,\n",
       " 4982,\n",
       " 1782,\n",
       " 2669,\n",
       " 417,\n",
       " 754,\n",
       " 2022,\n",
       " 3316,\n",
       " 3370,\n",
       " 347,\n",
       " 4283,\n",
       " 1005,\n",
       " 1279,\n",
       " 262,\n",
       " 851,\n",
       " 780,\n",
       " 1458,\n",
       " 4231,\n",
       " 4964,\n",
       " 3432,\n",
       " 3315,\n",
       " 6317,\n",
       " 6551,\n",
       " 4694,\n",
       " 5816,\n",
       " 2162,\n",
       " 2740,\n",
       " 2288,\n",
       " 6264,\n",
       " 907,\n",
       " 7246,\n",
       " 6151,\n",
       " 2032,\n",
       " 1583,\n",
       " 2869,\n",
       " 3869,\n",
       " 5353,\n",
       " 2115,\n",
       " 4666,\n",
       " 2219,\n",
       " 4896,\n",
       " 3426,\n",
       " 998,\n",
       " 5382,\n",
       " 5999,\n",
       " 3954,\n",
       " 4348,\n",
       " 1637,\n",
       " 7118,\n",
       " 3407,\n",
       " 6674,\n",
       " 1546,\n",
       " 2294,\n",
       " 4064,\n",
       " 714,\n",
       " 2926,\n",
       " 4411,\n",
       " 3123,\n",
       " 4206,\n",
       " 1620,\n",
       " 489,\n",
       " 4750,\n",
       " 606,\n",
       " 4362,\n",
       " 5543,\n",
       " 4450,\n",
       " 2581,\n",
       " 3037,\n",
       " 2372,\n",
       " 5412,\n",
       " 3584,\n",
       " 4592,\n",
       " 3797,\n",
       " 2942,\n",
       " 4331,\n",
       " 5209,\n",
       " 1723,\n",
       " 6132,\n",
       " 4685,\n",
       " 7092,\n",
       " 3060,\n",
       " 4385,\n",
       " 5106,\n",
       " 5272,\n",
       " 4481,\n",
       " 4107,\n",
       " 238,\n",
       " 564,\n",
       " 760,\n",
       " 54,\n",
       " 3469,\n",
       " 5770,\n",
       " 6271,\n",
       " 1308,\n",
       " 7131,\n",
       " 6753,\n",
       " 4039,\n",
       " 3923,\n",
       " 5615,\n",
       " 4014,\n",
       " 1391,\n",
       " 4690,\n",
       " 5794,\n",
       " 4300,\n",
       " 3972,\n",
       " 501,\n",
       " 1579,\n",
       " 2920,\n",
       " 6224,\n",
       " 2378,\n",
       " 481,\n",
       " 2975,\n",
       " 4121,\n",
       " 5490,\n",
       " 5277,\n",
       " 3532,\n",
       " 3374,\n",
       " 2982,\n",
       " 5964,\n",
       " 2956,\n",
       " 3703,\n",
       " 5146,\n",
       " 1414,\n",
       " 4115,\n",
       " 4113,\n",
       " 3066,\n",
       " 3783,\n",
       " 3537,\n",
       " 718,\n",
       " 476,\n",
       " 1505,\n",
       " 3108,\n",
       " 6229,\n",
       " 2788,\n",
       " 1309,\n",
       " 1581,\n",
       " 7025,\n",
       " 17,\n",
       " 4108,\n",
       " 6000,\n",
       " 6350,\n",
       " 3160,\n",
       " 2953,\n",
       " 3288,\n",
       " 4043,\n",
       " 1261,\n",
       " 6943,\n",
       " 6406,\n",
       " 3786,\n",
       " 6021,\n",
       " 3906,\n",
       " 5153,\n",
       " 1320,\n",
       " 6080,\n",
       " 6834,\n",
       " 3138,\n",
       " 1925,\n",
       " 734,\n",
       " 2522,\n",
       " 985,\n",
       " 2450,\n",
       " 1364,\n",
       " 5367,\n",
       " 5636,\n",
       " 6847,\n",
       " 799,\n",
       " 297,\n",
       " 4245,\n",
       " 4813,\n",
       " 6158,\n",
       " 1839,\n",
       " 4020,\n",
       " 1228,\n",
       " 5135,\n",
       " 1204,\n",
       " 4110,\n",
       " 5422,\n",
       " 3520,\n",
       " 5843,\n",
       " 2579,\n",
       " 674,\n",
       " 3605,\n",
       " 2312,\n",
       " 912,\n",
       " 2203,\n",
       " 6074,\n",
       " 2352,\n",
       " 2338,\n",
       " 7032,\n",
       " 3604,\n",
       " 4437,\n",
       " 1527,\n",
       " 1362,\n",
       " 4456,\n",
       " 4096,\n",
       " 4612,\n",
       " 5831,\n",
       " 3613,\n",
       " 4762,\n",
       " 2719,\n",
       " 3932,\n",
       " 430,\n",
       " 4795,\n",
       " 3955,\n",
       " 4951,\n",
       " 4068,\n",
       " 5245,\n",
       " 1377,\n",
       " 2237,\n",
       " 4072,\n",
       " 6429,\n",
       " 1156,\n",
       " 1425,\n",
       " 5563,\n",
       " 3893,\n",
       " 2147,\n",
       " 2595,\n",
       " 4112,\n",
       " 5068,\n",
       " 4839,\n",
       " 4312,\n",
       " 1531,\n",
       " 5479,\n",
       " 3387,\n",
       " 3854,\n",
       " 6409,\n",
       " 6537,\n",
       " 123,\n",
       " 6813,\n",
       " 3203,\n",
       " 5549,\n",
       " 4498,\n",
       " 1406,\n",
       " 2262,\n",
       " 3767,\n",
       " 3507,\n",
       " 7153,\n",
       " 2714,\n",
       " 6913,\n",
       " 3901,\n",
       " 3104,\n",
       " 6792,\n",
       " 304,\n",
       " 6473,\n",
       " 4699,\n",
       " 4030,\n",
       " 1181,\n",
       " 1779,\n",
       " 5895,\n",
       " 6567,\n",
       " 1738,\n",
       " 6832,\n",
       " 4722,\n",
       " 2936,\n",
       " 3603,\n",
       " 4050,\n",
       " 4299,\n",
       " 654,\n",
       " 5287,\n",
       " 1954,\n",
       " 4401,\n",
       " 3002,\n",
       " 1347,\n",
       " 6970,\n",
       " 292,\n",
       " 4474,\n",
       " 7160,\n",
       " 3761,\n",
       " 2697,\n",
       " 3051,\n",
       " 5060,\n",
       " 7133,\n",
       " 4611,\n",
       " 3529,\n",
       " 1004,\n",
       " 5423,\n",
       " 519,\n",
       " 5070,\n",
       " 4335,\n",
       " 1799,\n",
       " 820,\n",
       " 6788,\n",
       " 5108,\n",
       " 3515,\n",
       " 1695,\n",
       " 3430,\n",
       " 3147,\n",
       " 911,\n",
       " 5270,\n",
       " 6413,\n",
       " 5939,\n",
       " 1523,\n",
       " 3648,\n",
       " 661,\n",
       " 6778,\n",
       " 6933,\n",
       " 1050,\n",
       " 6023,\n",
       " 4719,\n",
       " 4920,\n",
       " 2389,\n",
       " 4593,\n",
       " 7247,\n",
       " 533,\n",
       " 4022,\n",
       " 2528,\n",
       " 5557,\n",
       " 318,\n",
       " 421,\n",
       " 847,\n",
       " 461,\n",
       " 3137,\n",
       " 4339,\n",
       " 553,\n",
       " 2474,\n",
       " 3543,\n",
       " 5957,\n",
       " 281,\n",
       " 1477,\n",
       " 4591,\n",
       " 7091,\n",
       " 4370,\n",
       " 4652,\n",
       " 4036,\n",
       " 5139,\n",
       " 3416,\n",
       " 4006,\n",
       " 4809,\n",
       " 3818,\n",
       " 6696,\n",
       " 6309,\n",
       " 1139,\n",
       " 5797,\n",
       " 488,\n",
       " 6726,\n",
       " 402,\n",
       " 4306,\n",
       " 645,\n",
       " 5833,\n",
       " 2593,\n",
       " 6427,\n",
       " 2899,\n",
       " 375,\n",
       " 3844,\n",
       " 358,\n",
       " 4557,\n",
       " 4630,\n",
       " 2569,\n",
       " 1060,\n",
       " 855,\n",
       " 1808,\n",
       " 6532,\n",
       " 3464,\n",
       " 2610,\n",
       " 5371,\n",
       " 4188,\n",
       " 1521,\n",
       " 3457,\n",
       " 2200,\n",
       " 6562,\n",
       " 2289,\n",
       " 1901,\n",
       " 3670,\n",
       " 5312,\n",
       " 1013,\n",
       " 6092,\n",
       " 247,\n",
       " 6422,\n",
       " 1472,\n",
       " 2321,\n",
       " 1818,\n",
       " 6687,\n",
       " 5660,\n",
       " 2689,\n",
       " 200,\n",
       " 834,\n",
       " 6055,\n",
       " 472,\n",
       " 5169,\n",
       " 1071,\n",
       " 1712,\n",
       " 5838,\n",
       " 3934,\n",
       " 5730,\n",
       " 4421,\n",
       " 5863,\n",
       " 6379,\n",
       " 7241,\n",
       " 1282,\n",
       " 5175,\n",
       " 2121,\n",
       " 307,\n",
       " 743,\n",
       " 3815,\n",
       " 1878,\n",
       " 2552,\n",
       " 6479,\n",
       " 2403,\n",
       " 4192,\n",
       " 6729,\n",
       " 702,\n",
       " 1300,\n",
       " 2187,\n",
       " 800,\n",
       " 3366,\n",
       " 6838,\n",
       " 5314,\n",
       " 6016,\n",
       " 2658,\n",
       " 980,\n",
       " 5848,\n",
       " 2980,\n",
       " 6641,\n",
       " 4357,\n",
       " 4541,\n",
       " 1988,\n",
       " 4152,\n",
       " 3421,\n",
       " 1997,\n",
       " 3134,\n",
       " 2578,\n",
       " 7265,\n",
       " 2969,\n",
       " 6706,\n",
       " 6599,\n",
       " 1409,\n",
       " 3317,\n",
       " 2139,\n",
       " 6592,\n",
       " 5871,\n",
       " 3291,\n",
       " 2437,\n",
       " 4566,\n",
       " 3350,\n",
       " 1692,\n",
       " 4911,\n",
       " 3574,\n",
       " 6689,\n",
       " 2622,\n",
       " 2347,\n",
       " 6303,\n",
       " 4176,\n",
       " 3287,\n",
       " 6019,\n",
       " 4767,\n",
       " 5174,\n",
       " 2978,\n",
       " 2987,\n",
       " 3292,\n",
       " 2912,\n",
       " 6476,\n",
       " 6191,\n",
       " 665,\n",
       " 3959,\n",
       " 202,\n",
       " 1264,\n",
       " 4222,\n",
       " 1383,\n",
       " 5477,\n",
       " 3662,\n",
       " 6094,\n",
       " 3376,\n",
       " 2635,\n",
       " 3386,\n",
       " 2897,\n",
       " 865,\n",
       " 7046,\n",
       " 3349,\n",
       " 6182,\n",
       " 3664,\n",
       " 617,\n",
       " 3227,\n",
       " 4817,\n",
       " 3470,\n",
       " 4398,\n",
       " 697,\n",
       " 1769,\n",
       " 2043,\n",
       " 2922,\n",
       " 7288,\n",
       " 5123,\n",
       " 1468,\n",
       " 5104,\n",
       " 4212,\n",
       " 6170,\n",
       " 5544,\n",
       " 259,\n",
       " 2909,\n",
       " 4615,\n",
       " 836,\n",
       " 1313,\n",
       " 814,\n",
       " 2036,\n",
       " 3139,\n",
       " 6299,\n",
       " 612,\n",
       " 6949,\n",
       " 1795,\n",
       " 1379,\n",
       " 4012,\n",
       " 1416,\n",
       " 1851,\n",
       " 2889,\n",
       " 100,\n",
       " 5892,\n",
       " 1656,\n",
       " 3046,\n",
       " 3977,\n",
       " 88,\n",
       " 1352,\n",
       " 684,\n",
       " 3189,\n",
       " 3712,\n",
       " 6015,\n",
       " 5331,\n",
       " 5522,\n",
       " 5432,\n",
       " 2859,\n",
       " 3607,\n",
       " 4764,\n",
       " 2333,\n",
       " 4842,\n",
       " 4475,\n",
       " 148,\n",
       " 2315,\n",
       " 1126,\n",
       " 3362,\n",
       " 4766,\n",
       " 2361,\n",
       " 6107,\n",
       " 4558,\n",
       " 4195,\n",
       " 1103,\n",
       " 2571,\n",
       " 7292,\n",
       " 2023,\n",
       " 2198,\n",
       " 1168,\n",
       " 2020,\n",
       " 4524,\n",
       " 4918,\n",
       " 5778,\n",
       " 3863,\n",
       " 719,\n",
       " 4579,\n",
       " 880,\n",
       " 4603,\n",
       " 3351,\n",
       " 1506,\n",
       " 1893,\n",
       " 216,\n",
       " 4400,\n",
       " 1848,\n",
       " 2947,\n",
       " 1340,\n",
       " 4448,\n",
       " 3124,\n",
       " 681,\n",
       " 5916,\n",
       " 2445,\n",
       " 667,\n",
       " 3145,\n",
       " 4436,\n",
       " 7180,\n",
       " 3884,\n",
       " 1504,\n",
       " 1090,\n",
       " 6447,\n",
       " 5297,\n",
       " 5156,\n",
       " 864,\n",
       " 5884,\n",
       " 5278,\n",
       " 4146,\n",
       " 3473,\n",
       " 1086,\n",
       " 2638,\n",
       " 1481,\n",
       " 3527,\n",
       " 1150,\n",
       " 3062,\n",
       " 2052,\n",
       " 6417,\n",
       " 5722,\n",
       " 1877,\n",
       " 3107,\n",
       " 571,\n",
       " 131,\n",
       " 4458,\n",
       " 4966,\n",
       " 6618,\n",
       " 6801,\n",
       " 4040,\n",
       " 7064,\n",
       " 3611,\n",
       " 1800,\n",
       " 3963,\n",
       " 5779,\n",
       " 6694,\n",
       " 3308,\n",
       " 5381,\n",
       " 162,\n",
       " 4140,\n",
       " 21,\n",
       " 6382,\n",
       " 4645,\n",
       " 2107,\n",
       " 5358,\n",
       " 3159,\n",
       " 5335,\n",
       " 5932,\n",
       " 3026,\n",
       " 3251,\n",
       " 2630,\n",
       " 5919,\n",
       " 5375,\n",
       " 4021,\n",
       " 1329,\n",
       " 6748,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 리뷰에 영향을 미치는 단어:\n",
      "--------------------------------------\n",
      "delicious\n",
      "amazing\n",
      "pleasantly\n",
      "fantastic\n",
      "excellent\n",
      "great\n",
      "awesome\n",
      "vegas\n",
      "perfect\n",
      "yummy\n",
      "solid\n",
      "superb\n",
      "love\n",
      "yum\n",
      "ngreat\n",
      "outstanding\n",
      "heaven\n",
      "perfection\n",
      "rocks\n",
      "best\n",
      "====\n",
      "\n",
      "\n",
      "\n",
      "부정 리뷰에 영향을 미치는 단어:\n",
      "--------------------------------------\n",
      "worst\n",
      "mediocre\n",
      "bland\n",
      "horrible\n",
      "awful\n",
      "terrible\n",
      "rude\n",
      "meh\n",
      "disgusting\n",
      "overpriced\n",
      "tasteless\n",
      "disappointing\n",
      "poorly\n",
      "unprofessional\n",
      "lacked\n",
      "underwhelmed\n",
      "poor\n",
      "unfriendly\n",
      "elsewhere\n",
      "slowest\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 긍정적인 상위 20개 단어\n",
    "print(\"긍정 리뷰에 영향을 미치는 단어:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")\n",
    "\n",
    "# 부정적인 상위 20개 단어\n",
    "print(\"부정 리뷰에 영향을 미치는 단어:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
