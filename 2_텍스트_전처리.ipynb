{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOh3FTqhL9bqz8YDIUu5CNN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "참고자료: 딥 러닝을 이용한 자연어 처리 입문      \n",
        "https://wikidocs.net/21667"
      ],
      "metadata": {
        "id": "newZgy9jaVq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [토큰화(Tokenization)]\n",
        "크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전치리 되지 않은 상태라면, 용도에 맞게 토큰화, 정제, 정규화를 해줘야 한다. 그 중 토큰화에 대해 학습해보자."
      ],
      "metadata": {
        "id": "8_XdDQ3ZbDLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 단어 토큰화(Word Tokenization)\n",
        "보통 토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제(Cleaning) 작업을 수행하는 것만으로 해결되지 않음.     \n",
        "구두점이나 특수문자를 모두 제거해버리면 의미를 잃어버리는 경우가 많기 때문임."
      ],
      "metadata": {
        "id": "FYEVy98tbLlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 토큰화 중 생기는 선택의 순간\n",
        "예상하지 못하는 경우가 있어 토큰화의 기준을 생각해봐야 함.    \n",
        "예) Don't, Don t, Dont, Don, ... "
      ],
      "metadata": {
        "id": "PQhigTL0biew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MtDuWSc1cw0Q",
        "outputId": "fc1e94d2-fe49-4fe0-abdf-e64ebaf8dd14"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lookup error 시 다운로드하기 \n",
        "# nltk.download()"
      ],
      "metadata": {
        "id": "iBVnKAQyefDi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer    # 어포스트로피 처리 확인하기 위함\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "e-Dk9I58cevD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUwzXHpa-Zko",
        "outputId": "c51417e0-aabd-4f45-ea02-c84e0478292a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery goes for a pastry shop.\"\n",
        "print(\"word_tokenize 이용 단어 토큰화: \", word_tokenize(text))\n",
        "print(\"\")\n",
        "print(\"WordPunctTokenizer 이용 단어 토큰화: \", WordPunctTokenizer().tokenize(text))\n",
        "print(\"\")\n",
        "print(\"Keras의 text_to_word_sequendce 이용 단어 토근화: \", text_to_word_sequence(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iopmt13vdHeS",
        "outputId": "6bea0a4e-dcfd-47f9-f07d-3b4209b4b589"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_tokenize 이용 단어 토큰화:  ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "\n",
            "WordPunctTokenizer 이용 단어 토큰화:  ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "\n",
            "Keras의 text_to_word_sequendce 이용 단어 토근화:  [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 토큰화에서 고려해야할 사항\n",
        "1) 구두점이나 특수 문자를 단순 제외해서는 안 됨     \n",
        "2) 줄임말과 단어 내에 띄어쓰기가 있는 경우    "
      ],
      "metadata": {
        "id": "poJS6zgyeCZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 표준 토큰화 예제: Treebank Tokenization\n",
        "- 하이픈으로 구성된 단어는 하나로 유지함 \n",
        "- doesn't와 같이 아포스트로피 접어가 함께하는 단어는 분리함"
      ],
      "metadata": {
        "id": "NpLopajaf9vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "text = \"Starting a home-based restaurant may be an ideal. It doesn't have a food chain or restaurant of their own.\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print(\"treebank workTokenizer 이용 단어 토큰화: \", tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "sZyArYGp4Rii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8b2abf-71b7-4fda-8378-5a0e33becdcc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "treebank workTokenizer 이용 단어 토큰화:  ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'It', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 문장 토큰화(Sentence Tokenization)\n",
        "갖고있는 corpus 내에서 문장 단위로 구분하는 작업, sentence segmentaion이라고도 함     \n",
        "단순히 마침표로만 문장을 구분짓지 않음.     \n",
        "예) Ph.D, IP 110.123.123.345 와 같은 단어 때문"
      ],
      "metadata": {
        "id": "AnY4_WKUkGXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He lookeds about, to make sure no one was near.\"\n",
        "print(\"sent_tokenize를 이용한 문장 토큰화: \", sent_tokenize(text))"
      ],
      "metadata": {
        "id": "pDBAOLCT45_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e261f7bb-21a2-42fa-ef1d-ad3030074dba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sent_tokenize를 이용한 문장 토큰화:  ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He lookeds about, to make sure no one was near.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 문장 토큰화\n",
        "!pip install kss"
      ],
      "metadata": {
        "id": "4ayhXvSg6Tw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e83b8ca5-3809-462f-9216-7ae157293889"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kss\n",
            "  Downloading kss-3.6.4.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 109 kB/s \n",
            "\u001b[?25hCollecting emoji==1.2.0\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2022.6.2)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from kss) (9.0.0)\n",
            "Building wheels for collected packages: kss\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.6.4-py3-none-any.whl size=42448613 sha256=ca3848b066d92a9a9b5d59cb873fb7d65a4626ea4c3ee53a2891eef440cd1ab0\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/53/7c/76bdf098c2a6c5cd4c4c29648da30ad4793d604314b2aeb26f\n",
            "Successfully built kss\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.2.0 kss-3.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss \n",
        "text = \"딥러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?\"\n",
        "print(\"한국어 문장 토큰화: \", kss.split_sentences(text))\n",
        "\n",
        "# 그치만 한국어 자연어처리는 굉장히 어렵다"
      ],
      "metadata": {
        "id": "NFW8STKG6q7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89405d95-78bd-44f3-c656-103df44ad106"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국어 문장 토큰화:  ['딥러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 품사 태깅(Part-of-Speech tagging)과 실습"
      ],
      "metadata": {
        "id": "ex371b4R8SLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCFaQaEcFoSG",
        "outputId": "86b7ec87-201c-467d-83d5-e8689541ac78"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize  # 토크나이저\n",
        "from nltk.tag import pos_tag   # 품사 태깅 \n",
        "\n",
        "text = \"I am actively looking for Ph.D. students. And you are a Ph.D. student.\"\n",
        "tokenized_sentence = word_tokenize(text)\n",
        "\n",
        "print(\"단어 토큰화: \", tokenized_sentence)\n",
        "print(\"품사 태깅: \", pos_tag(tokenized_sentence))"
      ],
      "metadata": {
        "id": "EBqCiAR38qZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8ee592-8163-4ea1-bcda-d2a25f994e34"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화:  ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'And', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "품사 태깅:  [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('And', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PRP: 인칭대명사\n",
        "- VBP: 동사\n",
        "- RB: 부사\n",
        "- VBG: 현재부사\n",
        "- IN: 전치사\n",
        "- NNP: 고유명사\n",
        "- NNS: 복수형 명사\n",
        "- CC: 접속사\n",
        "- DT: 관사"
      ],
      "metadata": {
        "id": "AtFA5Uee9TCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "eds2rjb4-5Vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72665ab-9766-4c0f-f944-a0b3ed2423f3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[K     |████████████████████████████████| 465 kB 72.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 형태소 분석하려면 KoNLPy 패키지 설치 후 Okt, Mecab, Komoran, Hannanum, Kkma 등 사용\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma  \n",
        "\n",
        "okt = Okt()\n",
        "kkma = Kkma()\n",
        "\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "print(\"OKT 형태소 분석: \", okt.morphs(text))\n",
        "print(\"OKT 품사 태깅: \", okt.pos(text))\n",
        "print(\"OKT 명사 추출: \", okt.nouns(text))"
      ],
      "metadata": {
        "id": "C6XXW_pk93Pl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c65a56-d1ef-4fea-d7c9-1541235a24d9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석:  ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
            "OKT 품사 태깅:  [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
            "OKT 명사 추출:  ['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [정제(Cleanign)와 정규화(Normalization)]\n",
        "토큰화 작업 전후에는 텍스트 데이터를 용도에 맞게 정제(cleaning)과 정규화(normalization)가 필요함. \n",
        "1. 정제: 갖고 있는 코퍼스로부터 노이즈 데이터를 제거 \n",
        "2. 정규화: 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어줌"
      ],
      "metadata": {
        "id": "1CsWNWuT_iwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 규칙에 기반한 표기가 다른 단어들의 통합"
      ],
      "metadata": {
        "id": "CGr5t301BTeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 대소문자 통합"
      ],
      "metadata": {
        "id": "tLT0Nm0itpAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 불필요한 단어의 제거\n",
        "1. 등장 빈도가 적은 단어\n",
        "2. 길이가 짧은 단어 \n",
        "\n",
        "정규표현식으로 추출 가능"
      ],
      "metadata": {
        "id": "o1-LhohRvob2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 정규 표현식"
      ],
      "metadata": {
        "id": "cdTH1ebEvryY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [어간 추출(Stemming)과 표제어 추출(Lemmatization)]\n",
        "정규화 기법 중 코퍼스에 있는 단어의 개수를 줄일 수 있는 기법"
      ],
      "metadata": {
        "id": "TfBSptO7v6e_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 표제어 추출(Lemmatization)\n",
        "표제어 또는 기본 사전형 단어.     \n",
        "단어들이 다른 형태를 가지더라도 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단.     \n",
        "예) am, are, is의 표제어는 be     \n",
        "\n",
        "1. 어간(Stem): 단어의 의미를 담고 있는 단어의 핵심 부분\n",
        "2. 접사(affix): 단어에 추가적인 의미를 주는 부분"
      ],
      "metadata": {
        "id": "DI2sorhCxHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatize lookup 에러 시 다운로드 필요\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "euiKhPJf2hDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369dd964-b346-4274-c4b3-b49389b8816e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print('표제어 추출 전: ', words)\n",
        "print(\"\")\n",
        "print('표제어 추출 후: ', \n",
        "      [lemmatizer.lemmatize(word) for word in words])\n",
        "# 여기선 ing/ed 표현 안없어지네"
      ],
      "metadata": {
        "id": "VFXyKbQFxrQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89bad7e2-09cd-40db-8cce-69c24e70de89"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표제어 추출 전:  ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "\n",
            "표제어 추출 후:  ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 동사라는 거 알려주면 뽑아줌\n",
        "print(lemmatizer.lemmatize('dies', 'v'))  \n",
        "print(lemmatizer.lemmatize('watched', 'v'))  \n",
        "print(lemmatizer.lemmatize('has', 'v'))"
      ],
      "metadata": {
        "id": "mi2wE5eD2Bzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b98eba5-9e24-4036-c9c0-587ac76a42f1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "die\n",
            "watch\n",
            "have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 어간 추출(Stemming)\n",
        "형태학적 분석을 단순화한 버전 또는 정해진 규칙을 이용해 단어의 어미를 자르는 어림짐작의 작업.    \n",
        "Porter Algorithm은 어간 추출 알고리즘 중 하나."
      ],
      "metadata": {
        "id": "I-WXTbG92WVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Porter 알고리즘"
      ],
      "metadata": {
        "id": "BoWLxQjMoLoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This was not the map we found in Billy Bone's chest, but an accurate copy, complete in all things -- names and heights amd soundings -- with the single exception of the red crosses and the written notes.\""
      ],
      "metadata": {
        "id": "bpJXbEqEhedX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lookup 에러시 \n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "XLk_NbcYm_Jt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf2f922-43be-41d7-830a-0a6466530859"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = \"This was not the map we found in Billy Bone's chest, but an accurate copy, complete in all things -- names and heights amd soundings -- with the single exception of the red crosses and the written notes.\"\n",
        "tokenized_sentence = word_tokenize(sentence)\n",
        "\n",
        "print(\"어간 추출 전 토큰화만 진행: \", tokenized_sentence)\n",
        "print(\"어간 추출 후: \", [stemmer.stem(word) for word in tokenized_sentence])\n",
        "# 규칙기반의 알고리즘이므로 어간 추출 후의 결과는 사전에 없는 단어들도 포함되어 있음. "
      ],
      "metadata": {
        "id": "nRHKbBuZmI2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f77aba1-f12d-4205-c07e-7d0a966fe35c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 토큰화만 진행:  ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bone', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'amd', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
            "어간 추출 후:  ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'amd', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 포터 알고리즘의 규칙 기반 어간 추출 시 \n",
        "words = ['formalize', 'allowance', 'electricical']\n",
        "print('어간 추출 전: ', words )\n",
        "print('어간 추출 후: ', [stemmer.stem(word) for word in words])"
      ],
      "metadata": {
        "id": "7zWu8vCzmmBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d200046-8e15-4ca0-be92-fa938ea77845"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전:  ['formalize', 'allowance', 'electricical']\n",
            "어간 추출 후:  ['formal', 'allow', 'electric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Porter 알고리즘 vs. Lancaster 알고리즘"
      ],
      "metadata": {
        "id": "3MP5Tk0Dn_LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print(\"Porter stemmer의 어간 추출 후: \", [porter_stemmer.stem(word) for word in words])\n",
        "print(\"Lancaster stemmer의 어간 추출 후: \", [lancaster_stemmer.stem(word) for word in words])"
      ],
      "metadata": {
        "id": "rqzS6DgvoUIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe759f7d-3c95-4eef-e521-c358e0d71762"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter stemmer의 어간 추출 후:  ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
            "Lancaster stemmer의 어간 추출 후:  ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 한국어에서의 어간 추출"
      ],
      "metadata": {
        "id": "8BaFgVImpHNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 한국어의 5언 9품\n",
        "\n",
        "|언|품사|\n",
        "|-----|----------|\n",
        "|체언|명사, 대명사, 수사|\n",
        "|수식언|관형사, 부사|\n",
        "|관계언|조사|\n",
        "|독립언|감탄사|\n",
        "|용언|동사, 형옹사|"
      ],
      "metadata": {
        "id": "l39pB24Yz4N6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [불용어(Stopword)]    \n",
        "갖고있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해 불용어는 제거해야함.     \n",
        "자주 등장하지만 분석을 하는 것에 있어서 큰 도움이 되지 않는 단어.    \n",
        "  -  예) I, my, over, 조사, 접미사 등"
      ],
      "metadata": {
        "id": "1ZNmGgjF0XCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "!pip install konlpy\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "v3Je1tK400t1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0291541-27ab-4832-e48d-63f85c3a6c6e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. NLTK에서 불용어 확인하기"
      ],
      "metadata": {
        "id": "k0QVAPV22rsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "stop_words_list = stopwords.words(\"english\")\n",
        "print(\"불용어 개수: \", len(stop_words_list))\n",
        "print(\"불용어 출력: \", stop_words_list[:10])"
      ],
      "metadata": {
        "id": "BVSPRVOI3UCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e26f8d3-c8ff-4705-bd53-963e71a348dd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수:  179\n",
            "불용어 출력:  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. NLTK를 통해서 불용어 제거하기"
      ],
      "metadata": {
        "id": "XVvDnQOj3ac6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VwrU_uTF-SS",
        "outputId": "137ddf67-f12d-45a2-8377-350296f17232"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"불용어 개수: \", len(stop_words))"
      ],
      "metadata": {
        "id": "LkXZXa5h3teO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e151013-ee15-45d2-ab7d-d37dc00f5a92"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수:  179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "result = []\n",
        "for word in word_tokens: \n",
        "  if word not in stop_words:\n",
        "    result.append(word)\n",
        "\n",
        "print(\"불용어 제거 전: \", word_tokens)\n",
        "print(\"불용어 제거 후: \", result)"
      ],
      "metadata": {
        "id": "bT9FRDjh3yhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b729839-4688-4eab-cd90-aba29cbea727"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전:  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후:  ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 한국어에서 불용어 제거하기"
      ],
      "metadata": {
        "id": "4Qttu6Ka72PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "text = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 에컨데 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "\n",
        "stop_words = set(stop_words.split(' '))\n",
        "word_tokens = okt.morphs(text)\n",
        "\n",
        "result = [word for word in word_tokens if not word in stop_words]\n",
        "print('불용어 제거 전: ', word_tokens)\n",
        "print('불용어 제거 후: ', result)"
      ],
      "metadata": {
        "id": "gZVysbQhAAjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6e7b67-fd84-4e53-9d53-1f044256a7da"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전:  ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '에컨데', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후:  ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '에컨데', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [정규표현식(Regular Expression)]\n",
        "- 파이썬 정규 표현식 모듈: re"
      ],
      "metadata": {
        "id": "eHTAuh5TAoWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 정규 표현식 실습"
      ],
      "metadata": {
        "id": "-66Rs0DTBQBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) . 기호: 한 개의 임의의 문자"
      ],
      "metadata": {
        "id": "ZncdtpV44d5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# re.compile: 정규표현식을 컴파일하는 함수"
      ],
      "metadata": {
        "id": "i-BvVKBdE4Tp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) . 기호: 한 개의 임의의 문자"
      ],
      "metadata": {
        "id": "4ECKE3Xjpl9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"a.c\")\n",
        "print(r.search(\"kkk\"))\n",
        "print(r.search(\"abc\")) # match"
      ],
      "metadata": {
        "id": "8ZjjPAALE6Gs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17fe37e7-ed9b-4820-db05-d39d4c2f8835"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "<re.Match object; span=(0, 3), match='abc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) ? 기호: 물음표 앞의 문자가 있을 수도 있고 없을 수도 있음"
      ],
      "metadata": {
        "id": "a-RQI-ENFX4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab?c\")\n",
        "print(r.search(\"abbc\"))\n",
        "print(r.search(\"abc\")) # match\n",
        "print(r.search(\"ac\"))  # match"
      ],
      "metadata": {
        "id": "F9laGXeT4i1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56321293-160a-43d5-ca98-302cdf7dcf59"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "<re.Match object; span=(0, 3), match='abc'>\n",
            "<re.Match object; span=(0, 2), match='ac'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) * 기호: * 바로 앞의 문자가 0개 이상일 경우"
      ],
      "metadata": {
        "id": "MyWxxocV5jwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab*c\")  # * 앞에있는 가 0개 이상일 경우\n",
        "print(r.search(\"a\"))\n",
        "print(r.search(\"ab\"))    # match\n",
        "print(r.search('abc'))   # match\n",
        "print(r.search('abbbc')) # match"
      ],
      "metadata": {
        "id": "VlsmbR6750VX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a363d1-fb2e-4b10-ebab-6b1d2ac95c8e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 3), match='abc'>\n",
            "<re.Match object; span=(0, 5), match='abbbc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4) + 기호: + 바로 앞의 문자가 1개 이상인 경우"
      ],
      "metadata": {
        "id": "_lWonfSh7XoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab+c\")\n",
        "print(r.search('ac'))\n",
        "print(r.search('abc'))     # match\n",
        "print(r.search('abbbbc'))  # match"
      ],
      "metadata": {
        "id": "Hw6VZo9v7gcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26e3fdb-b523-45d4-8dad-7c18c03cb403"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "<re.Match object; span=(0, 3), match='abc'>\n",
            "<re.Match object; span=(0, 6), match='abbbbc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5) ^기호: 시작되는 문자열을 지정"
      ],
      "metadata": {
        "id": "vKr7M9iLpePR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile('^ab')\n",
        "print(r.search('bbc'))\n",
        "print(r.search('zab'))\n",
        "print(r.search('abzz'))  # match\n",
        "print(r.search('abzmkdflkajkntkjnlknhet')) # match"
      ],
      "metadata": {
        "id": "JRsxlHU9r-sD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0094b1d8-9e2a-4ee3-957f-d493983b472b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 2), match='ab'>\n",
            "<re.Match object; span=(0, 2), match='ab'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6) {숫자} 기호: 문자에 해당 기호를 붙이면 해당 숫자만큼 반복한 것을 나타냄"
      ],
      "metadata": {
        "id": "UoVjJdc7sQGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab{2}c\") # a와 c 사이에 b가 존재하면서 b가 두 개인 문자열\n",
        "print(r.search('ac'))\n",
        "print(r.search('abc'))\n",
        "print(r.search('abbc')) # match"
      ],
      "metadata": {
        "id": "BeBdqMAOsUvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f2e8d6-f60c-405c-b5ee-1e28207765f3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 4), match='abbc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7) {숫자1, 숫자2} 기호: 숫자 1 이상 숫자 2이하 반복"
      ],
      "metadata": {
        "id": "K31SHH0LY1p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab{2,8}c\")\n",
        "print(r.search('abbc'))"
      ],
      "metadata": {
        "id": "6XUdPL5KZ3Lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ee56fd-f46c-4daa-e12e-ab85e2bcb0c6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 4), match='abbc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab{2,8}c\")  # 헐 {2, 8}처럼 띄어쓰기 하면 안됨\n",
        "print(r.search('abc')) \n",
        "print(r.search('abbc')) # match\n",
        "print(r.search('abbbbbbbbbbbbbbbbc')) "
      ],
      "metadata": {
        "id": "gZIIFD6UY7sf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d077da0d-8aac-40da-9881-7f1af1cb9ff1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "<re.Match object; span=(0, 4), match='abbc'>\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8) {숫자, } 기호: "
      ],
      "metadata": {
        "id": "28OPXtfBZswK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"a{2,}bc\") # 띄어쓰기 안됨\n",
        "\n",
        "print(r.search(\"abc\"))\n",
        "print(r.search(\"aabc\"))  # match\n",
        "print(r.search(\"aaab\"))\n",
        "print(r.search(\"aaabc\"))  # match"
      ],
      "metadata": {
        "id": "v38PuZPkbsMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71715ebd-7997-496d-a97a-40429c239e60"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "<re.Match object; span=(0, 4), match='aabc'>\n",
            "None\n",
            "<re.Match object; span=(0, 5), match='aaabc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9) [ ] 기호 : 안에 문자들 중 한 개의 문자와 매치\n",
        "※ [a-zA-Z]는 알파벳 전부를 뜻함"
      ],
      "metadata": {
        "id": "37IJM2SHeB19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"[abc]\")\n",
        "print(r.search(\"zzz\"))\n",
        "print(r.search(\"zza\"))  # match\n",
        "print(r.search(\"\"))\n",
        "print(r.search(\"baac\"))  # match"
      ],
      "metadata": {
        "id": "oNwhrYd0eBgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecf0c08-c0be-4a26-b788-60955743dd7b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "<re.Match object; span=(2, 3), match='a'>\n",
            "None\n",
            "<re.Match object; span=(0, 1), match='b'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"[a-z]\")\n",
        "print(r.search(\"AAA\"))\n",
        "print(r.search(\"111F\"))\n",
        "print(r.search(\"1f1f\")) # match"
      ],
      "metadata": {
        "id": "8yp-8z7Re8nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e788965-3fdf-4b67-c5ed-ddc8fe1224c8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "<re.Match object; span=(1, 2), match='f'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 정규 표현식 모듈 함수 예제"
      ],
      "metadata": {
        "id": "rSo9p0vjfJ2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. re.match()와 re.search()"
      ],
      "metadata": {
        "id": "DcIBaOOOfi1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile('ab.') \n",
        "print(r.match('kkkabc'))\n",
        "print(r.match('abckkk')) # match는 처음 시작하는 부분과 비교"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJYaVSnlEjta",
        "outputId": "9a20ee81-002c-48df-bd13-64bcfebf7749"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "<re.Match object; span=(0, 3), match='abc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(r.search('kkkabc'))\n",
        "print(r.search('abckkk'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gNTm3EOGWi5",
        "outputId": "9bce04d0-40a5-4d28-f4ee-9c0c9a9c1d51"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(3, 6), match='abc'>\n",
            "<re.Match object; span=(0, 3), match='abc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. re.split()"
      ],
      "metadata": {
        "id": "F92RFm214b3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"사과 딸기 수박 메론 바나나\"\n",
        "re.split(\" \", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRN5YF-cGnxO",
        "outputId": "6585b639-5c28-4e35-b9eb-6a3dd26bdedf"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사과', '딸기', '수박', '메론', '바나나']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"사과\n",
        "딸기\n",
        "수박\n",
        "메로 ㄴ\n",
        "바나나ㅏ\"\"\"\n",
        "re.split(\"\\n\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsEnel73Gx49",
        "outputId": "8adabcc2-bf52-4141-f3c5-5f97ee644061"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사과', '딸기', '수박', '메로 ㄴ', '바나나ㅏ']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. re.findall()  \n",
        "매치되는 모든 문자열을 리스트로 반환"
      ],
      "metadata": {
        "id": "Towq9wthHDIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"이름: 황소\n",
        "전화번호: 010-1111-2222 \n",
        "나이: 30\n",
        "성별: 여\"\"\"\n",
        "\n",
        "text2 = \"안녕하세염????\"\n",
        "print(re.findall('\\d+', text))   # d는 숫자 \n",
        "print(re.findall('\\d+', text2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki1vA-QyK0tF",
        "outputId": "5441fb38-d31f-4771-f9f4-0977916a4e8e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['010', '1111', '2222', '30']\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. re.sub()\n",
        "패턴과 일치하는 부분을 대체"
      ],
      "metadata": {
        "id": "O85Ey6CbLR76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Regular Expression: A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
        "\n",
        "preprocessed_text = re.sub('[^a-zA-Z]', ' ', text)  # 알파벳이 아닌 것들을 공백으로 치환\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2VLwqUPLibv",
        "outputId": "6ce4a746-1e68-4225-e4cb-71f9fca7460d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular Expression  A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. 예제"
      ],
      "metadata": {
        "id": "ChzJSUWsMQoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"100 John    PROF\n",
        "101 James   STUD\n",
        "102 Mac   STUD\"\"\"\n",
        "\n",
        "print(re.findall('[A-Z]{4}', text)) # 대문자 연속으로 네 글자"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I-36C14MB6V",
        "outputId": "26dce4f0-4e1c-4102-941a-81d71a2f7694"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PROF', 'STUD', 'STUD']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8FY0qR21N9Bi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}