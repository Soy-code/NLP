{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피드 포워드 신경망\n",
    "\n",
    "* 다층 퍼셉트론(Multi Layer Perceptron): 간단한 퍼셉트론을 구조적으로 확장한 신경망. 많은 퍼셉트론이 있는 층을 여러 개 쌓아올린 구조.\n",
    "* CNN: 디지털 신호 처리에 사용하는 윈도우 필터(window filter)에 영향을 받아 만든 신경망. 컴퓨터 비전에 적합하고 단어나 문장 같은 순차 데이터에서 부분 구조를 감지하는 데도 이상적.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "* 기본적인 신경망 구성 요소\n",
    "* 입력으로 데이터 벡터를 받고 출력값 하나를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "seed = 1337\n",
    "torch.manual_seed(seed) # 파이토치에서 random seed를 고정하기 위한 함수로 manual_seed 제공\n",
    "torch.cuda.manual_seed_all(seed) # gpu(2개 이상) 사용할 때 random 값 생성 \n",
    "\n",
    "# print(torch.rand(5))\n",
    "\n",
    "# torch.manual_seed(seed)\n",
    "# print(torch.rand(5))\n",
    "\n",
    "# torch.manual_seed(seed)\n",
    "# print(torch.rand(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp class\n",
    "class MultiLayerPerceptron(nn.Module) :\n",
    "    def __init__(self, input_dim:int, hidden_dim:int, output_dim:int) :\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): 입력 벡터 크기\n",
    "            hidden_dim (int): 첫 번째 Linear 층의 출력 크기\n",
    "            output_dim (int): 두 번째 Linear 층의 출력 크기\n",
    "        \"\"\"\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x_in, apply_softmax=False) :\n",
    "        \"\"\"MLP의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 데이터 텐서\n",
    "                x_in.shape는 (batch, input_dim)입니다.\n",
    "            apply_softmax (bool): 소프트맥스 활성화 함수를 위한 플래그\n",
    "                크로스-엔트로피 손실을 사용하려면 False로 지정해야 합니다.\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape은 (batch, output_dim)입니다.\n",
    "        \"\"\"\n",
    "        intermediate = F.relu(self.fc1(x_in))  # 첫 번째 linear 층에 렐루 활성화 함수가 적용됨 \n",
    "                                               # 한 층의 출력 개수는 다음 층의 입력 개수와 같아야 함\n",
    "                                               # 두 linear층 사이에 놓인 비선형 활성화 함수는 필수 --> 복잡한 패턴을 모델링할 수 없음\n",
    "        output = self.fc2(intermediate) \n",
    "        \n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (fc1): Linear(in_features=3, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# mlp 객체 생성\n",
    "batch_size = 2\n",
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "output_dim = 4\n",
    "\n",
    "mlp = MultiLayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([2, 3])\n",
      "값: \n",
      "tensor([[0.9989, 0.4964, 0.4362],\n",
      "        [0.7803, 0.3707, 0.1563]])\n",
      "------------------------------------------------------------------------------------------\n",
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([2, 100])\n",
      "값: \n",
      "tensor([[ 0.8093, -1.0344,  0.6736,  0.0363,  0.8183,  0.2733,  0.5725,  0.0081,\n",
      "          0.8957,  1.1066,  0.5748,  0.0803,  0.1496, -0.0837,  0.6955, -0.8565,\n",
      "          1.3087,  1.0789, -0.2564,  0.1304, -0.2649,  0.3614,  0.1451, -0.2292,\n",
      "         -0.3187,  0.2484, -0.0904, -0.1061,  0.2381,  0.0687, -0.3042, -0.2283,\n",
      "         -0.5808,  0.4489, -0.2671, -0.0181,  0.1161, -0.9804,  0.2172,  0.4448,\n",
      "         -0.6551,  0.3006,  0.3126, -0.4494,  0.2932, -0.7768, -0.6186, -0.5880,\n",
      "         -0.1581, -0.4465, -0.7421, -0.2253, -0.2683, -0.0608, -0.1055, -0.2919,\n",
      "          0.3970, -0.6066,  0.4848, -0.8993,  0.8466,  0.7402, -0.4736, -0.7543,\n",
      "          0.5531,  0.0898, -0.8098,  0.7992,  0.8099,  0.6066, -0.3838, -0.3546,\n",
      "         -0.2685,  0.3117,  0.5397,  0.5661, -0.0721,  0.8285, -0.8483,  0.5774,\n",
      "          0.0773, -0.6509, -0.4631,  0.0314,  0.1724, -0.0819, -0.5494, -0.1010,\n",
      "         -0.1466, -0.2459,  0.5600, -0.5879, -0.4044, -0.5579, -0.0937, -1.1311,\n",
      "         -0.1935,  0.3227, -0.0798,  0.0357],\n",
      "        [ 0.7871, -0.9052,  0.5030,  0.0598,  0.6832,  0.1513,  0.4633, -0.1186,\n",
      "          0.8742,  0.9239,  0.6004,  0.2489,  0.1850, -0.2520,  0.5782, -0.6355,\n",
      "          0.9977,  0.9659, -0.3145, -0.0118, -0.2143,  0.2809,  0.2641, -0.2367,\n",
      "         -0.1355,  0.0695, -0.0438, -0.0383,  0.3095,  0.0527, -0.1669, -0.1867,\n",
      "         -0.4276,  0.4114, -0.2068, -0.0239,  0.0629, -0.8070,  0.3114,  0.3677,\n",
      "         -0.6597,  0.0574,  0.2574, -0.3632,  0.0996, -0.7610, -0.5686, -0.4389,\n",
      "         -0.1893, -0.4111, -0.6457,  0.0045, -0.2959, -0.0222, -0.2301, -0.3158,\n",
      "          0.4645, -0.5641,  0.4128, -0.8951,  0.7439,  0.5485, -0.2527, -0.6977,\n",
      "          0.5127, -0.0276, -0.7749,  0.6158,  0.6185,  0.3937, -0.3897, -0.3598,\n",
      "         -0.1197,  0.4632,  0.3934,  0.5232, -0.1991,  0.6475, -0.8555,  0.4582,\n",
      "         -0.0616, -0.6099, -0.3848, -0.0131,  0.1310, -0.0238, -0.4311, -0.0767,\n",
      "         -0.1021, -0.0640,  0.5206, -0.4120, -0.2537, -0.6162,  0.0348, -0.8774,\n",
      "         -0.1262,  0.2286, -0.1405, -0.0533]], grad_fn=<AddmmBackward0>)\n",
      "------------------------------------------------------------------------------------------\n",
      "타입: torch.FloatTensor\n",
      "크기: torch.Size([2, 4])\n",
      "값: \n",
      "tensor([[ 0.1458, -0.3921,  0.1217,  0.2616],\n",
      "        [ 0.1455, -0.3759,  0.1534,  0.2180]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def describe(x) :\n",
    "    print(\"---\" * 30)\n",
    "    print(\"타입: {}\".format(x.type()))\n",
    "    print(\"크기: {}\".format(x.shape))\n",
    "    print(\"값: \\n{}\".format(x))\n",
    "    \n",
    "# 입력 데이터\n",
    "x_input = torch.rand(batch_size, input_dim)\n",
    "describe(x_input)\n",
    "\n",
    "# 첫 번째 linear 층 출력\n",
    "fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "intermediate = fc1(x_input)\n",
    "describe(intermediate)\n",
    "\n",
    "# linear output에 활성화함수 적용값 출력\n",
    "intermediate = F.relu()\n",
    "\n",
    "# 두 번째 linear층 출력\n",
    "fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "output = fc2(intermediate)\n",
    "describe(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
